"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9462],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return m}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var i=a.createContext({}),c=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(i.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,s=e.originalType,i=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=c(n),m=r,h=d["".concat(i,".").concat(m)]||d[m]||p[m]||s;return n?a.createElement(h,o(o({ref:t},u),{},{components:n})):a.createElement(h,o({ref:t},u))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=n.length,o=new Array(s);o[0]=d;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var c=2;c<s;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3864:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return i},metadata:function(){return c},toc:function(){return u},default:function(){return d}});var a=n(3117),r=n(102),s=(n(7294),n(3905)),o=["components"],l={id:"3_Configure_server_storage",title:"Configure Storage Server"},i=void 0,c={unversionedId:"Kubernetes/kubernetes_storage_&_scheduling/3_Configure_server_storage",id:"Kubernetes/kubernetes_storage_&_scheduling/3_Configure_server_storage",title:"Configure Storage Server",description:"Install and configure nfs-kerner-server",source:"@site/docs/Kubernetes/3_kubernetes_storage_&_scheduling/3_Configure_server_storage.md",sourceDirName:"Kubernetes/3_kubernetes_storage_&_scheduling",slug:"/Kubernetes/kubernetes_storage_&_scheduling/3_Configure_server_storage",permalink:"/docs/Kubernetes/kubernetes_storage_&_scheduling/3_Configure_server_storage",editUrl:"https://github.com/j-peguet/portfolio/blob/master/docs/Kubernetes/3_kubernetes_storage_&_scheduling/3_Configure_server_storage.md",tags:[],version:"current",lastUpdatedAt:1613556017,formattedLastUpdatedAt:"2/17/2021",sidebarPosition:3,frontMatter:{id:"3_Configure_server_storage",title:"Configure Storage Server"},sidebar:"docs",previous:{title:"Define PV & PVC",permalink:"/docs/Kubernetes/kubernetes_storage_&_scheduling/2_Define_PV_&_PVC"},next:{title:"Storage Class",permalink:"/docs/Kubernetes/kubernetes_storage_&_scheduling/4_Storage_Class"}},u=[{value:"Demo 1",id:"demo-1",children:[],level:2},{value:"Demo 2",id:"demo-2",children:[],level:2}],p={toc:u};function d(e){var t=e.components,n=(0,r.Z)(e,o);return(0,s.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"Install and configure nfs-kerner-server"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Install NFS Server and create the directory for our exports\nsudo apt install -y nfs-kerner-server\nsudo mkdir -p /export/volumes/pod\n\n#Configure our NFS Export in /etc/export for /export/volumes. Using no_root_squash and no_subtree_check to \n#allow applications to mount subdirectories of the export directly.\nsudo bash -c 'echo \"/export/volumes  *(rw,no_root_squash,no_subtree_check)\" > /etc/exports'\ncat /etc/exports\nsudo systemctl restart nfs-kernel-server.service\nexit\n")),(0,s.kt)("p",null,"On each Node (not the master server), install the NFS Client"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt install -y nfs-common\n")),(0,s.kt)("p",null,"Test the installation on one Node. If this test don't work create persistent volume in Kubernetes cannot be possible."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Try to mount a volume\nsudo mount -t nfs4 c1-storage1:/export/volumes /mnt/\nmount | grep nfs\n#Unmount it\nsudo umount /mnt\nexit\n")),(0,s.kt)("h2",{id:"demo-1"},"Demo 1"),(0,s.kt)("p",null,"Static Provisioning Persistent Volumes"),(0,s.kt)("p",null,"create a file nfs.pv.yaml"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-nfs-data\nspec:\n  accessModes:\n    - ReadWriteMany\n  capacity:\n    storage: 10Gi\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    server: 192.168.4.103\n    path: "/export/volumes/pod"\n')),(0,s.kt)("p",null,"and a nfs.pvc.yaml file"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-nfs-data\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Create a PV with the read/write many and retain as the reclaim policy\nkubectl apply -f nfs.pv.yaml\n\n\n#Review the created resources, Status, Access Mode and Reclaim policy is set to Reclaim rather than Delete. \nkubectl get PersistentVolume pv-nfs-data\n\n#Look more closely at the PV and it's configuration\nkubectl describe PersistentVolume pv-nfs-data\n\n#Create a PVC on that PV\nkubectl apply -f nfs.pvc.yaml\n\n\n#Check the status, now it's Bound due to the PVC on the PV. See the claim...\nkubectl get PersistentVolume\n\n#Check the status, Bound.\n#We defined the PVC it statically provisioned the PV...but it's not mounted yet.\nkubectl get PersistentVolumeClaim pvc-nfs-data\nkubectl describe PersistentVolumeClaim pvc-nfs-data\n")),(0,s.kt)("p",null,"Create some content in our storage server"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"ssh jules@c1-storage1\nsudo bash -c 'echo \"Hello from our NFS mount!!!\" > /export/volumes/pod/demo.html'\ncat /export/volumes/pod/demo.html\nexit\n")),(0,s.kt)("p",null,"Create a nfs.nginx.yaml file"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nfs-deployment\nspec:  \n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      volumes:\n      - name: webcontent\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: webcontent\n          mountPath: "/usr/share/nginx/html/web-app"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-nfs-service\nspec:\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Let's create a Pod (in a Deployment and add a Service) with a PVC on pvc-nfs-data\nkubectl apply -f nfs.nginx.yaml\nkubectl get service nginx-nfs-service\n#Create an environment variable, that containt the IP address of the service\nSERVICEIP=$(kubectl get service | grep nginx-nfs-service | awk '{ print $3 }')\n\n#Check to see if our pods are Running before proceeding\nkubectl get pods\n\n#Let's access that application to see our application data...\ncurl http://$SERVICEIP/web-app/demo.html\n")),(0,s.kt)("p",null,"Check the mouted volume in the container"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Check the Mounted By output for which Pod(s) are accessing this storage\nkubectl describe PersistentVolumeClaim pvc-nfs-data\n \n#If we go 'inside' the Pod/Container, let's look at where the PV is mounted\nkubectl exec -it nginx-nfs-deployment-[tab][tab] -- /bin/bash\nls /usr/share/nginx/html/web-app\ncat /usr/share/nginx/html/web-app/demo.html\nexit\n")),(0,s.kt)("p",null,"Check the mouted volume on the Node"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#What node is this pod on?\nkubectl get pods -o wide\n\n#Let's log into that node and look at the mounted volumes...it's the kubelets job to make the device/mount available.\nssh c1-node[X]\nmount | grep nfs\nexit\n")),(0,s.kt)("p",null,"Delete the pod, can we get our data ?"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Let's delete the pod and see if we still have access to our data in our PV...\nkubectl get pods\nkubectl delete pods nginx-nfs-deployment-[tab][tab]\n\n#We get a new pod...but is our app data still there???\nkubectl get pods\n\n\n#Let's access that application to see our application data...yes!\ncurl http://$SERVICEIP/web-app/demo.html\n")),(0,s.kt)("h2",{id:"demo-2"},"Demo 2"),(0,s.kt)("p",null,"Control the Persistent Volume with Access Modes (Demo 1 is required)."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#scale up the deployment to 4 replicas\nkubectl scale deployment nginx-nfs-deployment --replicas=4\n\n#Now let's look at who's attached to the pvc, all 4 Pods\n#Our AccessMode for this PV and PVC is RWX ReadWriteMany\nkubectl describe PersistentVolumeClaim \n\n#Now when we access our application we're getting load balanced across all the pods hitting the same PV data\ncurl http://$SERVICEIP/web-app/demo.html\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Let's delete our deployment\nkubectl delete deployment nginx-nfs-deployment\n\n#Check status, still bound on the PV...why is that...\nkubectl get PersistentVolume \n\n#Because the PVC still exists...\nkubectl get PersistentVolumeClaim\n\n#Can re-use the same PVC and PV from a Pod definition...yes! Because I didn't delete the PVC.\nkubectl apply -f nfs.nginx.yaml\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#Our app is up and running\nkubectl get pods \n\n#But if I delete the deployment\nkubectl delete deployment nginx-nfs-deployment\n\n#AND I delete the PersistentVolumeClaim\nkubectl delete PersistentVolumeClaim pvc-nfs-data\n\n#My status is now Released...which means no one can claim this PV\nkubectl get PersistentVolume\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#But let's try to use it and see what happend, recreate the PVC for this PV\nkubectl apply -f nfs.pvc.yaml\n\n#Then try to use the PVC/PV in a Pod definition\nkubectl apply -f nfs.nginx.yaml\n\n#My pod creation is Pending\nkubectl get pods\n\n#As is my PVC Status...Pending...because that PV is released and our Reclaim Policy is Retain\nkubectl get PersistentVolumeClaim\nkubectl get PersistentVolume\n\n#Need to delete the PV if we want to 'reuse' that exact PV...to 're-create' the PV\nkubectl delete deployment nginx-nfs-deployment\nkubectl delete pvc pvc-nfs-data\nkubectl delete pv pv-nfs-data\n")),(0,s.kt)("p",null,"Now if I come back and recreate the PV, then I can get access to the underlying storage again with this deployment and its PVC and this process of deleting that PV puts that decision of do I want to be able to reuse that storage again into the administrator's hands ",(0,s.kt)("strong",{parentName:"p"},"Kubernetes doesnt automatically make this decision for you.")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"#If we recreate the PV, PVC, and the pods. we'll be able to re-deploy. \n#The clean up of the data is defined by the reclaim policy. (Delete will clean up for you, useful in dynamic provisioning scenarios)\n#But in this case, since it's NFS, we have to clean it up and remove the files\n#Nothing will prevent a user from getting this acess to this data, so it's imperitive to clean up. \nkubectl apply -f nfs.pv.yaml\nkubectl apply -f nfs.pvc.yaml\nkubectl apply -f nfs.nginx.yaml\nkubectl get pods \n\n\n#Time to clean up for the next demo\nkubectl delete -f nfs.nginx.yaml\nkubectl delete pvc pvc-nfs-data\nkubectl delete pv pv-nfs-data\n")))}d.isMDXComponent=!0}}]);