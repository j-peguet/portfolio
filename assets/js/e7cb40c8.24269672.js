"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7313],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return h}});var o=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},l=Object.keys(e);for(o=0;o<l.length;o++)n=l[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(o=0;o<l.length;o++)n=l[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var a=o.createContext({}),u=function(e){var t=o.useContext(a),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d=function(e){var t=u(e.components);return o.createElement(a.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},p=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,a=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),p=u(n),h=r,g=p["".concat(a,".").concat(h)]||p[h]||c[h]||l;return n?o.createElement(g,s(s({ref:t},d),{},{components:n})):o.createElement(g,s({ref:t},d))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,s=new Array(l);s[0]=p;var i={};for(var a in t)hasOwnProperty.call(t,a)&&(i[a]=t[a]);i.originalType=e,i.mdxType="string"==typeof e?e:r,s[1]=i;for(var u=2;u<l;u++)s[u]=n[u];return o.createElement.apply(null,s)}return o.createElement.apply(null,n)}p.displayName="MDXCreateElement"},8336:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return a},metadata:function(){return u},toc:function(){return d},default:function(){return p}});var o=n(3117),r=n(102),l=(n(7294),n(3905)),s=["components"],i={id:"7_Scheduling",title:"Scheduling"},a=void 0,u={unversionedId:"Kubernetes/kubernetes_storage_&_scheduling/7_Scheduling",id:"Kubernetes/kubernetes_storage_&_scheduling/7_Scheduling",title:"Scheduling",description:"Scheduling inside Kubernetes, is the part of the master Node (he has the responsibility for scheduling workload).",source:"@site/docs/Kubernetes/3_kubernetes_storage_&_scheduling/7_Scheduling.md",sourceDirName:"Kubernetes/3_kubernetes_storage_&_scheduling",slug:"/Kubernetes/kubernetes_storage_&_scheduling/7_Scheduling",permalink:"/docs/Kubernetes/kubernetes_storage_&_scheduling/7_Scheduling",editUrl:"https://github.com/j-peguet/portfolio/blob/master/docs/Kubernetes/3_kubernetes_storage_&_scheduling/7_Scheduling.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{id:"7_Scheduling",title:"Scheduling"},sidebar:"docs",previous:{title:"Accessing a Private Container Registry",permalink:"/docs/Kubernetes/kubernetes_storage_&_scheduling/6_Access_to_private_registry"},next:{title:"Control Scheduling",permalink:"/docs/Kubernetes/kubernetes_storage_&_scheduling/8_0_Control_scheduling"}},d=[{value:"Kubernetes has one job...",id:"kubernetes-has-one-job",children:[],level:3},{value:"starting Pods on Nodes",id:"starting-pods-on-nodes",children:[],level:3},{value:"Scheduling in Kubernetes",id:"scheduling-in-kubernetes",children:[],level:2},{value:"The goal of Scheduling",id:"the-goal-of-scheduling",children:[],level:2},{value:"Scheduling Process",id:"scheduling-process",children:[],level:2},{value:"Node Selection",id:"node-selection",children:[],level:2},{value:"Resources Request",id:"resources-request",children:[],level:2},{value:"Demo 1",id:"demo-1",children:[],level:2},{value:"Demo 2",id:"demo-2",children:[],level:2}],c={toc:d};function p(e){var t=e.components,i=(0,r.Z)(e,s);return(0,l.kt)("wrapper",(0,o.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"Scheduling inside Kubernetes, is the part of the master Node (he has the responsibility for scheduling workload)."),(0,l.kt)("h3",{id:"kubernetes-has-one-job"},"Kubernetes has one job..."),(0,l.kt)("h3",{id:"starting-pods-on-nodes"},"starting Pods on Nodes"),(0,l.kt)("p",null,"But to so this, a lot of decisions have to be made (enough resources, policies in the cluster)."),(0,l.kt)("h2",{id:"scheduling-in-kubernetes"},"Scheduling in Kubernetes"),(0,l.kt)("p",null,"Scheduler is the process of selecting a node to start a Pod on inside of our Clutser"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Selecting a Node to start a Pod on - one of the core components in the master node"),(0,l.kt)("li",{parentName:"ul"},"kube-scheduler - this is the default scheduler"),(0,l.kt)("li",{parentName:"ul"},"you can create our own scheduler")),(0,l.kt)("h2",{id:"the-goal-of-scheduling"},"The goal of Scheduling"),(0,l.kt)("p",null,"Is find the best, most feasible node in the cluster to run a pod."),(0,l.kt)("p",null,"To make this decisions, he have two things:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Resources - cpu, memory, storage"),(0,l.kt)("li",{parentName:"ul"},"Policy - ex: two Pod on different nodes or zones")),(0,l.kt)("h2",{id:"scheduling-process"},"Scheduling Process"),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"scheduling process",src:n(3215).Z})),(0,l.kt)("h2",{id:"node-selection"},"Node Selection"),(0,l.kt)("p",null,"Let's look more closely on the node selection process."),(0,l.kt)("p",null,"The node selection have 3 phases:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Filetering - remove node that cannot run our pod",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"From all the nodes"),(0,l.kt)("li",{parentName:"ul"},"Apply filters"),(0,l.kt)("li",{parentName:"ul"},"Filtered Nodes"),(0,l.kt)("li",{parentName:"ul"},"Hard constraints - pod spec, node resources, etc..."),(0,l.kt)("li",{parentName:"ul"},"If no node is found, the pod is unscheduable and will fail scheduling"))),(0,l.kt)("li",{parentName:"ul"},"Scoring - process of yielding a list of eligible nodes",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Scoring function - Start from the filtered list"),(0,l.kt)("li",{parentName:"ul"},"Feasible Nodes"),(0,l.kt)("li",{parentName:"ul"},"Policu constraints - image on the node, etc..."),(0,l.kt)("li",{parentName:"ul"},"Find the best node to run our pod"))),(0,l.kt)("li",{parentName:"ul"},"Binding - updating the node name in our pods object",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Selected Nodes List - the highest priority nodes"),(0,l.kt)("li",{parentName:"ul"},"Ties are broken - if they are multiple nodes, a random was selected"),(0,l.kt)("li",{parentName:"ul"},"Update API Object")))),(0,l.kt)("h2",{id:"resources-request"},"Resources Request"),(0,l.kt)("p",null,"Resource are important in the process of shoosing a node"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Setting requests will cause the scheduler to find a Node to fit the workload/Pod",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"requests are guarantees"),(0,l.kt)("li",{parentName:"ul"},"CPU"),(0,l.kt)("li",{parentName:"ul"},"Memory"))),(0,l.kt)("li",{parentName:"ul"},"Allocatable resources per Node"),(0,l.kt)("li",{parentName:"ul"},"Pods that need to be scheduled but there not enough resources available will go Pending")),(0,l.kt)("h2",{id:"demo-1"},"Demo 1"),(0,l.kt)("p",null,"Finding scheduling information"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"#Let's create a deployment with three replicas\nkubectl apply -f deployment.yaml\n\n#Pods spread out evenly across the Nodes due to our scoring functions for selector spread during Scoring.\nkubectl get pods -o wide\n\n#We can look at the Pods events to see the scheduler making its choice\nkubectl describe pods \n\n#If we scale our deployment to 6...\nkubectl scale deployment hello-world --replicas=6\n\n#We can see that the scheduler works to keep load even across the nodes.\nkubectl get pods -o wide\n\n#We can see the nodeName populated for this node\nkubectl get pods hello-world-[tab][tab] -o yaml\n\n#Clean up this demo...and delete its resources\nkubectl delete deployment hello-world\n")),(0,l.kt)("h2",{id:"demo-2"},"Demo 2"),(0,l.kt)("p",null,"Scheduling Pods with resource requests. "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"#Start a watch, the pods will go from Pending->ContainerCreating->Running\n#Each pod has a 1 core CPU request.\nkubectl get pods --watch &\nkubectl apply -f requests.yaml\n\n#We created three pods, one but one is not starting...\nkubectl get pods -o wide\n\n#Let's scale our deployment to 6 replica.  These pods will stay pending.  Some pod names may be repeated.\nkubectl scale deployment hello-world-requests --replicas=6\n\n#We see that three Pods are pending...why?\nkubectl get pods -o wide\nkubectl get pods -o wide | grep Pending\n\n#Let's look at why the Pod is Pending...check out the Pod's events...\nkubectl describe pods\n\n#Now let's look at the node's Allocations...we've allocated 62% of our CPU...\n#1 User pod using 1 whole CPU, one system Pod using 250 millicores of a CPU and \n#looking at allocatable resources, we have only 2 whole Cores available for use.\n#The next pod coming along wants 1 whole core, and tha'ts not available.\n#The scheduler can't find a place in this cluster to place our workload...is this good or bad?\nkubectl describe node c1-node1\n\n#Clean up after this demo\nkubectl delete deployment hello-world-requests\n\n#stop the watch\nfg\nctrl+c\n")))}p.isMDXComponent=!0},3215:function(e,t,n){t.Z=n.p+"assets/images/scheduling_process-f1e8f998749ef2a055f772fb09810ddd.png"}}]);